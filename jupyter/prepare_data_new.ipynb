{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  filter out known/unknown results according to short answers only based on no search results\n",
    "datasets=[\"asqa\", \"nq\", \"trivia-qa\", \"hotpot-qa\"]\n",
    "split=('train')\n",
    "chat_model=\"llama7b\"\n",
    "search_engine=\"kiltbm25\"\n",
    "rerank_model=\"e5base\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "  def remove_articles(text):\n",
    "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def calculate_short_answer_EM(generated_answer, gold_answers):\n",
    "    generated_answer=normalize_answer(generated_answer)\n",
    "    match=0\n",
    "    for gold_answer in gold_answers:\n",
    "        gold_answer=normalize_answer(gold_answer)\n",
    "        if gold_answer in generated_answer:\n",
    "            match+=1\n",
    "    return {\n",
    "        \"recall\": match/len(gold_answers),\n",
    "        \"exact_match\": match>0\n",
    "    }\n",
    "      \n",
    "for dataset in datasets:\n",
    "    # print(f\"dataset: {dataset}, split: {split}, chat_model: {chat_model}, search_engine: {search_engine}, rerank_model: {rerank_model}\")\n",
    "    #  no search chat results\n",
    "    if split == \"trainmore\":\n",
    "        without_search_chat_results_file=f'../user_intent_data/{dataset}/{chat_model}/without_search/{chat_model}-{dataset}-train.jsonl'\n",
    "        without_search_chat_results_file2=f'../user_intent_data/{dataset}/{chat_model}/without_search/{chat_model}-{dataset}-trainmore.jsonl'\n",
    "        if os.path.exists(without_search_chat_results_file2):\n",
    "            without_search_chat_results=[json.loads(line) for line in open(without_search_chat_results_file, \"r\", encoding=\"utf-8\")]\n",
    "            without_search_chat_results.extend([json.loads(line) for line in open(without_search_chat_results_file2, \"r\", encoding=\"utf-8\")])\n",
    "        else:\n",
    "            without_search_chat_results=[json.loads(line) for line in open(without_search_chat_results_file, \"r\", encoding=\"utf-8\")]\n",
    "    else:\n",
    "        without_search_chat_results_file=f'../user_intent_data/{dataset}/{chat_model}/without_search/{chat_model}-{dataset}-{split}.jsonl'\n",
    "        without_search_chat_results=[json.loads(line) for line in open(without_search_chat_results_file, \"r\", encoding=\"utf-8\")]\n",
    "    generated_answers=[data_line[f\"{chat_model}_without_search_answer\"] for data_line in without_search_chat_results]\n",
    "    \n",
    "    if len(without_search_chat_results) == 0:\n",
    "        continue\n",
    "    if \"short_answers\" in without_search_chat_results[0]:\n",
    "        gold_answers=[data_line[\"short_answers\"] for data_line in without_search_chat_results]\n",
    "    else:\n",
    "        if isinstance(without_search_chat_results[0][\"answer\"], list):\n",
    "            gold_answers=[data_line[\"answer\"] for data_line in without_search_chat_results]\n",
    "        else:\n",
    "            gold_answers=[[data_line[\"answer\"]] for data_line in without_search_chat_results]\n",
    "    \n",
    "    without_search_em_results = [calculate_short_answer_EM(generated_answers[idx], gold_answers[idx]) for idx in range(len(generated_answers))]\n",
    "    \n",
    "        \n",
    "    filtered_gpt4_rewrite=[]\n",
    "    search_tags=[]\n",
    "    sample_count=0\n",
    "\n",
    "    thresholds={\n",
    "        \"asqa\": 0.5,\n",
    "        \"qampari\": 0.7,\n",
    "        \"nq\": 0.5,\n",
    "        \"trivia-qa\": 0.5,\n",
    "        \"2wiki\": 0.5,\n",
    "        \"musique\": 0.5,\n",
    "        \"hotpot-qa\": 0.5,\n",
    "    }\n",
    "    threshold=thresholds[dataset]\n",
    "    for idx in range(len(without_search_em_results)):\n",
    "        without_search_chat_results[idx][\"dataset\"]=dataset\n",
    "        if without_search_em_results[idx][\"recall\"] > threshold:\n",
    "            search_tag=\"<known> \"\n",
    "            without_search_chat_results[idx][\"known\"] = True\n",
    "            sample_count+=1\n",
    "        else:\n",
    "            search_tag=\"<unknown> \"\n",
    "            without_search_chat_results[idx][\"known\"] = False\n",
    "        # if gpt4_rewrite_em_results[idx][\"recall\"] < vanilla_search_em_results[idx][\"recall\"]:\n",
    "        #     search_tag+=\"<original>\"\n",
    "        #     without_search_chat_data[idx][\"rewrite\"] = False\n",
    "        # else:\n",
    "        #     search_tag+=\"<rewrite>\"\n",
    "        #     without_search_chat_data[idx][\"rewrite\"] = True\n",
    "        #  drop some samples if recall are all equal\n",
    "        # if without_search_em_results[idx][\"recall\"] == vanilla_search_em_results[idx][\"recall\"] and without_search_em_results[idx][\"recall\"] == gpt4_rewrite_em_results[idx][\"recall\"]:\n",
    "        #     continue\n",
    "        # unknow samples are no more than known samples\n",
    "        if \"unknown\" in search_tag:\n",
    "            if sample_count > 0:\n",
    "                sample_count-=1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        search_tags.append(search_tag)\n",
    "        # without_search_chat_data[idx][\"search_tags\"] = search_tag\n",
    "        filtered_gpt4_rewrite.append(without_search_chat_results[idx])\n",
    "        #  filtered sample should be no more than 4000\n",
    "        # filtered_gpt4_rewrite=filtered_gpt4_rewrite[:5000]\n",
    "        #  make sure there are no duplicated ids\n",
    "        filtered_ids=[data_line[\"id\"] for data_line in filtered_gpt4_rewrite]\n",
    "        assert len(filtered_ids) == len(set(filtered_ids))\n",
    "        \n",
    "    print(f\"filtered {len(filtered_gpt4_rewrite)} data, total {len(without_search_chat_results)} data\")\n",
    "    print(f\"dataset: {dataset}, split: {split}, chat_model: {chat_model}, search_engine: {search_engine}, rerank_model: {rerank_model}\")\n",
    "    search_tag_num_dict={k: search_tags.count(k) for k in set(search_tags)}\n",
    "    print(search_tag_num_dict)\n",
    "    print(f\"dropped {len(without_search_chat_results)-len(filtered_gpt4_rewrite)} samples\")\n",
    "    if not os.path.exists(f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/\"):\n",
    "        os.makedirs(f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/\", exist_ok=True)\n",
    "    #  save filtered results to mixed dataset\n",
    "    mixed_dataset_file=f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/filtered-{dataset}-{split}.jsonl\"\n",
    "    with open(mixed_dataset_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx in range(len(filtered_gpt4_rewrite)):\n",
    "            f.write(json.dumps(filtered_gpt4_rewrite[idx], ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#  filter training data for SKR\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "random.seed(4545)\n",
    "\n",
    "datasets=(\"asqa\", \"nq\", \"trivia-qa\", \"musique\")\n",
    "split=\"trainfew\"\n",
    "chat_model=\"qwen72b\"\n",
    "search_engine=\"kiltbm25\"\n",
    "rerank_model=\"e5base\"\n",
    "\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "  def remove_articles(text):\n",
    "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def calculate_short_answer_EM(generated_answer, gold_answers):\n",
    "    generated_answer=normalize_answer(generated_answer)\n",
    "    match=0\n",
    "    for gold_answer in gold_answers:\n",
    "        gold_answer=normalize_answer(gold_answer)\n",
    "        if gold_answer in generated_answer:\n",
    "            match+=1\n",
    "    return {\n",
    "        \"recall\": match/len(gold_answers),\n",
    "        \"exact_match\": match>0\n",
    "    }\n",
    "\n",
    "for dataset in datasets:\n",
    "    no_search_chat_results_file=f'../user_intent_data/{dataset}/{chat_model}/without_search/{chat_model}-{dataset}-{split}.jsonl'\n",
    "    no_search_chat_results=[json.loads(line) for line in open(no_search_chat_results_file, \"r\", encoding=\"utf-8\")]\n",
    "    if \"short_answers\" in no_search_chat_results[0]:\n",
    "        gold_answers=[data_line[\"short_answers\"] for data_line in no_search_chat_results]\n",
    "    else:\n",
    "        if isinstance(no_search_chat_results[0][\"answer\"], list):\n",
    "            gold_answers=[data_line[\"answer\"] for data_line in no_search_chat_results]\n",
    "        else:\n",
    "            gold_answers=[[data_line[\"answer\"]] for data_line in no_search_chat_results]\n",
    "            \n",
    "    no_search_generated_answers=[data_line[f\"{chat_model}_without_search_answer\"] for data_line in no_search_chat_results]\n",
    "    no_search_em_results = [calculate_short_answer_EM(no_search_generated_answers[idx], gold_answers[idx]) for idx in range(len(no_search_generated_answers))]\n",
    "    \n",
    "    vanilla_search_chat_results_file=f'../user_intent_data/{dataset}/{chat_model}/{search_engine}/vanilla_search/{rerank_model}-{chat_model}-{dataset}-{split}.jsonl'\n",
    "    vanilla_search_chat_results=[json.loads(line) for line in open(vanilla_search_chat_results_file, \"r\", encoding=\"utf-8\")]\n",
    "    vanilla_search_generated_answers=[data_line[f\"{chat_model}_vanilla_search_answer\"] for data_line in vanilla_search_chat_results]\n",
    "    vanilla_search_em_results = [calculate_short_answer_EM(vanilla_search_generated_answers[idx], gold_answers[idx]) for idx in range(len(vanilla_search_generated_answers))]\n",
    "    \n",
    "    #  filter out known/unknown results according to short answers\n",
    "    #  if no search results >= vanilla search results, then it is known\n",
    "    #  if no search results < vanilla search results, then it is unknown\n",
    "    filtered_gpt4_rewrite=[]\n",
    "    search_tags=[]\n",
    "    sample_count=0\n",
    "    if \"recall\" in no_search_em_results[0]:\n",
    "        for idx in range(len(no_search_em_results)):\n",
    "            if no_search_em_results[idx][\"recall\"] > vanilla_search_em_results[idx][\"recall\"]:\n",
    "                search_tag=\"<known> \"\n",
    "                no_search_chat_results[idx][\"known\"] = True\n",
    "                sample_count+=1\n",
    "            elif no_search_em_results[idx][\"recall\"] < vanilla_search_em_results[idx][\"recall\"]:\n",
    "                search_tag=\"<unknown> \"\n",
    "                no_search_chat_results[idx][\"known\"] = False\n",
    "            else:\n",
    "                continue\n",
    "            # if both recall are 0, then drop the sample\n",
    "            if no_search_em_results[idx][\"recall\"] == 0 and vanilla_search_em_results[idx][\"recall\"] == 0:\n",
    "                continue\n",
    "            filtered_gpt4_rewrite.append(no_search_chat_results[idx])\n",
    "            search_tags.append(search_tag)\n",
    "    elif \"rouge\" in no_search_em_results[0]:\n",
    "        for idx in range(len(no_search_em_results)):\n",
    "            if no_search_em_results[idx][\"rouge\"][\"rougeL\"] > vanilla_search_em_results[idx][\"rouge\"][\"rougeL\"]:\n",
    "                search_tag=\"<known> \"\n",
    "                no_search_chat_results[idx][\"known\"] = True\n",
    "                sample_count+=1\n",
    "            elif no_search_em_results[idx][\"rouge\"][\"rougeL\"] < vanilla_search_em_results[idx][\"rouge\"][\"rougeL\"]:\n",
    "                search_tag=\"<unknown> \"\n",
    "                no_search_chat_results[idx][\"known\"] = False\n",
    "            else:\n",
    "                continue\n",
    "    print(f\"filtered {len(filtered_gpt4_rewrite)} data\")\n",
    "    print(f\"dataset: {dataset}, split: {split}, chat_model: {chat_model}, search_engine: {search_engine}, rerank_model: {rerank_model}\")\n",
    "    search_tag_num_dict={k: search_tags.count(k) for k in set(search_tags)}\n",
    "    print(search_tag_num_dict)\n",
    "    if not os.path.exists(f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/\"):\n",
    "        os.makedirs(f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/\", exist_ok=True)\n",
    "    #  save filtered results to mixed dataset\n",
    "    mixed_dataset_file=f\"../user_intent_data/mixed/{dataset}/{chat_model}/{search_engine}/filtered-{dataset}-{split}.jsonl\"\n",
    "    with open(mixed_dataset_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx in range(len(filtered_gpt4_rewrite)):\n",
    "            f.write(json.dumps(filtered_gpt4_rewrite[idx], ensure_ascii=False) + \"\\n\")\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cc2edf60b276a9f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#  parse llama question rewrite & separate claim results\n",
    "import json\n",
    "datasets=(\"asqa\", \"nq\", \"trivia-qa\", \"musique\", \"eli5\")\n",
    "split=\"test\"\n",
    "chat_model=\"baichuan7b\"\n",
    "sep_model=\"v0118\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_file=f\"../user_intent_data/{dataset}/rewrite/{sep_model}{chat_model}/unparsed-{sep_model}{chat_model}-{dataset}-{split}.jsonl\"\n",
    "    output_file=f\"../user_intent_data/{dataset}/rewrite/{sep_model}{chat_model}/{sep_model}{chat_model}-{dataset}-{split}.jsonl\"\n",
    "    \n",
    "    with open(input_file, \"r\") as f:\n",
    "        data_lines=[json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "    for data_line in data_lines:\n",
    "        output_str = data_line[f\"{sep_model}{chat_model}_rewrite\"]\n",
    "        #  parse task\n",
    "        try:\n",
    "            task=output_str.split(\"<Task(\")[1].split(\")>\")[0]\n",
    "        except:\n",
    "            print(f\"parse timeliness error: {output_str}\")\n",
    "            task=\"\"\n",
    "        #  parse timeliness\n",
    "        try:\n",
    "            timeliness=output_str.split(\"<Timeliness(\")[1].split(\")>\")[0]\n",
    "            timeliness=True if timeliness==\"True\" else False\n",
    "        except:\n",
    "            print(f\"parse timeliness error: {output_str}\")\n",
    "            timeliness=False\n",
    "            \n",
    "        #  parse questions\n",
    "        try:\n",
    "            questions=output_str.split(\"<Questions>\")[1].split(\"</Questions>\")[0]\n",
    "            questions=[q for q in questions.split(\"<Question(\") if q.strip()]\n",
    "            rewrite_questions=[]\n",
    "    \n",
    "            for question_block in questions:\n",
    "                question=question_block.split(\")>\")[0]\n",
    "                need_search=question_block.split(\"<NeedSearch(\")[1].split(\")>\")[0]\n",
    "                need_search=True if need_search==\"True\" else False\n",
    "                search_word=question_block.split(\"<Query(\")[1].split(\")>\")[0]\n",
    "                rewrite_questions.append({\n",
    "                    \"question\": question,\n",
    "                    \"needSearch\": need_search,\n",
    "                    \"query\": search_word\n",
    "                })\n",
    "        except:\n",
    "            print(f\"parse questions error: {output_str}\")\n",
    "            rewrite_questions=[]\n",
    "            \n",
    "        #  parse claims\n",
    "        try:\n",
    "            claims=output_str.split(\"<Claims>\")[1].strip()\n",
    "            if claims.endswith(\"</Claims>\"):\n",
    "                claims=claims.split(\"</Claims>\")[0]\n",
    "            claims=[c for c in claims.split(\"<Claim(\") if c.strip()]\n",
    "            rewrite_claims=[]\n",
    "            for claim_block in claims:\n",
    "                claim_block=claim_block.strip()\n",
    "                if not claim_block.endswith(\")>\") or \"<NeedSearch(\" not in claim_block or \"<Query(\" not in claim_block:\n",
    "                    continue\n",
    "                claim=claim_block.split(\")>\")[0]\n",
    "                need_search=claim_block.split(\"<NeedSearch(\")[1].split(\")>\")[0]\n",
    "                need_search=True if need_search==\"True\" else False\n",
    "                query=claim_block.split(\"<Query(\")[1].split(\")>\")[0]\n",
    "                rewrite_claims.append({\n",
    "                    \"claim\": claim,\n",
    "                    \"needSearch\": need_search,\n",
    "                    \"query\": query\n",
    "                })\n",
    "        except:\n",
    "            print(f\"parse claims error: {output_str}\")\n",
    "            rewrite_claims=[]\n",
    "            \n",
    "        data_line[f\"{sep_model}{chat_model}_rewrite\"] = {\n",
    "            \"task\": task,\n",
    "            \"timeliness\": timeliness,\n",
    "            \"questions\": rewrite_questions,\n",
    "            \"claims\": rewrite_claims\n",
    "        }\n",
    "        \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in data_lines:\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "263faf49275ae92f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#  parse llama judge output strings \n",
    "from json import JSONDecodeError\n",
    "# datasets=(\"asqa\", \"hotpot-qa\", \"nq\", \"trivia-qa\", \"2wiki\", \"qampari\", \"musique\", \"dolly\", \"eli5\")\n",
    "datasets=(\"asqa\", \"nq\", \"trivia-qa\", \"musique\", \"eli5\")\n",
    "split='test'\n",
    "rewrite_model='v0104qwen7b'\n",
    "for dataset in datasets:\n",
    "    if dataset.startswith(\"v\"):\n",
    "        llama_rewrite_file=f'../user_intent_data/mixed/{dataset}/rewrite/{rewrite_model}/{rewrite_model}-{dataset}-{split}.jsonl'\n",
    "        parsed_llama_rewrite_file=f'../user_intent_data/mixed/{dataset}/rewrite/{rewrite_model}/unparsed-{rewrite_model}-{dataset}-{split}.jsonl'\n",
    "    else:\n",
    "        llama_rewrite_file=f'../user_intent_data/{dataset}/rewrite/{rewrite_model}/{rewrite_model}-{dataset}-{split}.jsonl'\n",
    "        parsed_llama_rewrite_file=f'../user_intent_data/{dataset}/rewrite/{rewrite_model}/unparsed-{rewrite_model}-{dataset}-{split}.jsonl'\n",
    "    with open(parsed_llama_rewrite_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        llama_rewrite_resutls = [json.loads(line) for line in f]\n",
    "        \n",
    "    #  example output:\n",
    "    #  {'generated_text': '<Task(information_extraction)> <Timeliness(True)> <Questions> <Question(What are the options for selecting Subsequent Payment terms for a Canadian mortgage?)> <KeyWords(Canadian mortgage Subsequent payment term options)> <NeedSearch(True)> </Questions> ', 'special_ret': {'tensor': []}}\n",
    "    for idx, data_line in enumerate(llama_rewrite_resutls):\n",
    "        output_str = data_line[f\"{rewrite_model}_rewrite\"]\n",
    "        try:\n",
    "            known=output_str.split(\"<Known(\")[1].split(\")>\")[0]\n",
    "            known=True if known==\"True\" else False\n",
    "        except:\n",
    "            print(f\"parse known error: {output_str}\")\n",
    "            known=False\n",
    "            \n",
    "        llama_rewrite_resutls[idx][f\"{rewrite_model}_rewrite\"] = {\n",
    "            \"known\": known,\n",
    "        }\n",
    "        \n",
    "            \n",
    "    with open(llama_rewrite_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in llama_rewrite_resutls:\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd98cda7b72c235"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
